<img src="./images/logo.png" width="350">

# Facial-Expression-Recognition-Zoo (FER-Zoo)

[![hugging](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Datasets-yellow)](https://huggingface.co/datasets/kdhht2334/DiffusionEmotion)
[![license](https://img.shields.io/badge/License-CC0/MIT-blue)](#licensing) <a href="https://releases.ubuntu.com/18.04/"><img alt="Ubuntu" src="https://img.shields.io/badge/Ubuntu-18.04-green"></a>
<a href="https://www.python.org/downloads/release/python-370/"><img alt="PyThon" src="https://img.shields.io/badge/Python-v3.8-blue"></a>
<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>

FER-Zoo is a PyTorch toolbox for facial expression recognition (FER). Especially, we focus on affect estimation methods to regress valence-arousal (VA) value. This repository contains state-of-the-art (SOTA) FER frameworks as follows:


| Methods | Venue | Link |
| --- | --- | --- |
| Baseline | TAC 2017 | [[link]](https://arxiv.org/abs/1708.03985) |
| CAF | AAAI 2021 | [[link]](https://ojs.aaai.org/index.php/AAAI/article/download/16743/16550) |
| AVCE | ECCV 2022 | [[link]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730181.pdf) |
| ELIM | NeurIPS 2022 | [[link]](https://arxiv.org/pdf/2209.12172) |

Also, we unveil the first `DiffusionFER` dataset for emotion analysis generated by `Stable Diffusion`. The diversity of this dataset collected directly by humans provides an opportunity to be used for privacy-related face applications such as deepfake, along with deep analysis between prompts and facial expression patterns.


What's New
---
- [Mar. 2023] Add open DiffusionExpression dataset created by `StableDiffusion-WebUI` [[link]](https://github.com/camenduru/stable-diffusion-webui-colab).
- [Mar. 2023] Add `Baseline` framework.
- [Mar. 2023] Add training and evaluation part of FER frameworks.
- [Mar. 2023] Initial version of FER-Zoo.


Requirements
---
* python >= 3.8.0
* pytorch >= 1.7.1
* torchvision >= 0.8.0
* pretrainedmodels >=0.7.4
* fabulous >= 0.4.0
* wandb > 0.13.0



Open Datasets: DiffusionFER
---

DiffusionFER is the large-scale text-to-image prompt database for face-related tasks. It contains about **1M(ongoing)** images generated by [Stable Diffusion](https://github.com/camenduru/stable-diffusion-webui-colab) using prompt(s) and other parameters. Dataset is available at [ðŸ¤— Hugging Datasets](https://huggingface.co/datasets/FER-Universe/DiffusionFER).


#### Subsets

DiffusionFER supports a total of three distinct splits. And, each split additionally provides a face region cropped by [face detector](https://github.com/timesler/facenet-pytorch).
   - DifussionEmotion_S (small), DifussionEmotion_M (medium), DifussionEmotion_L (large).

|Subset|Num of Images|Size|Image Directory |
|:--|--:|--:|--:|
|DifussionEmotion_S (original) | 1.5K | 647M | `DifussionEmotion_S/` |
|DifussionEmotion_S (cropped)  | 1.5K | 322M | `DiffusionEmotion_S_cropped/` |
|DifussionEmotion_M (original) | N/A  | N/A  | `DifussionEmotion_M/` |
|DifussionEmotion_M (cropped)  | N/A  | N/A  | `DiffusionEmotion_M_cropped/` |
|DifussionEmotion_L (original) | N/A  | N/A  | `DifussionEmotion_L/` |
|DifussionEmotion_L (cropped)  | N/A  | N/A  | `DiffusionEmotion_L_cropped/` |


#### Dataset Structure

We provide DiffusionFER using a modular file structure. `DiffusionEmotion_S`, the smallest scale, contains about 1,500 images and is divided into folders of a total of 7 emotion classes. The class labels of all these images are included in `dataset_sheet.csv`.
   - In `dataset_sheet.csv`, not only 7-emotion class but also _valence-arousal_ value are annotated.

```bash
# Small version of DB
./
â”œâ”€â”€ DifussionEmotion_S
â”‚Â Â  â”œâ”€â”€ angry
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ aaaaaaaa_6.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ andtcvhp_6.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ azikakjh_6.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [...]
â”‚Â Â  â”œâ”€â”€ fear
â”‚Â Â  â”œâ”€â”€ happy
â”‚Â Â  â”œâ”€â”€ [...]
â”‚Â Â  â””â”€â”€ surprise
â””â”€â”€ dataset_sheet.csv
```

```bash
# Medium version of DB
(ongoing)
```

```bash
# Large version of DB
(ongoing)
```

#### Data Loading (easy way)

DiffusionFER can be loaded via both Python and Git. Please refer Hugging Face [`Datasets`](https://huggingface.co/docs/datasets/quickstart).

```python
from datasets import load_dataset

dataset = load_dataset("FER-Universe/DiffusionFER")
```

```bash
git lfs install
git clone https://huggingface.co/datasets/FER-Universe/DiffusionFER
```
  - Facial regions are cropped by using `facenet-pytorch` [[link]](https://github.com/timesler/facenet-pytorch).
  
  - Change `<dataset_type>` to `custom` when training the model using `run.sh`.
  
  
#### Data Loading (handy way)

Download `.zip` folders [here](https://huggingface.co/datasets/FER-Universe/DiffusionFER).


Public Datasets
---

1. Download four public benchmarks for training and evaluation (please download after **agreement** accepted).

  - [AffectNet](http://mohammadmahoor.com/affectnet/)
  - [Aff-wild](https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge/) 
  - [Aff-wild2](https://ibug.doc.ic.ac.uk/resources/aff-wild2/)
  - [AFEW-VA](https://ibug.doc.ic.ac.uk/resources/afew-va-database/)
 
 (For more details visit [website](https://ibug.doc.ic.ac.uk/))

2. Follow preprocessing rules for each dataset by referring pytorch official [custom dataset tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).


Training
---

Just run the below script!
```
chmod 755 run.sh
./run.sh <dataset_type> <method> <gpu_no> <port_no> 
```
- `<dataset_type>`: 2 options (`custom` or `public`).
- `<method>`: 4 options (`elim`, `avce`, `caf`, and `baseline`).
- `<gpu_no>`: GPU number such as 0 (or 0, 1 etc.)
- `<port_no>`: port number to clarify workers (e.g., 12345)


Evaluation
---

- Evaluation is performed automatically at each `print_check` point in training phase.


Milestone
---
  - [x] Build SOTA FER frameworks
  - [ ] Upload pre-trained weights
  - [ ] Bench-marking table


Acknowledgments
This repository is partially inspired by [FaceX-Zoo](https://github.com/JDAI-CV/FaceX-Zoo) and [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch).

Citation
---

If our work is useful for your work, then please consider citing below bibtex:

```bibtex
@inproceedings{kim2021contrastive,
    title={Contrastive adversarial learning for person independent facial emotion recognition},
    author={Kim, Daeha and Song, Byung Cheol},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={35},
    number={7},
    pages={5948--5956},
    year={2021}
}

@inproceedings{kim2022emotion,
    title={Emotion-aware Multi-view Contrastive Learning for Facial Emotion Recognition},
    author={Kim, Daeha and Song, Byung Cheol},
    booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XIII},
    pages={178--195},
    year={2022},
    organization={Springer}
}

@misc{kim2022elim,
    author = {Kim, Daeha and Song, Byung Cheol},
    title = {Optimal Transport-based Identity Matching for Identity-invariant Facial Expression Recognition},
    Year = {2022},
    Eprint = {arXiv:2209.12172}
}
```

Contact (or collaborate)
---

If you have any questions or work with me, feel free to contact [Daeha Kim](kdhht5022@gmail.com).


Licensing
---

The ExpressionDiffusion dataset is available under the [CC0 1.0 License](https://creativecommons.org/publicdomain/zero/1.0/).
For the license of each FER framework, please refer to those of each conference society.